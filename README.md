# Papers
I like reading about Deep Learning and Computer Vision. Here I simply list the papers I read and that I want to read. Inspiration: [here](https://github.com/fregu856/papers).

## Taxonomy
I categorize papers by those I have read and those I have yet to read. Later, I will classify them based on if they are classics or not. Right now I am reading a mix of new and time tested papers.

‚úÖ Paper I have read.

üöÄ Exceptional read.

‚ùå Paper I have yet to read.

----

## 2025

### Self-supervised learning
* ‚úÖ 03/2025 [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
* ‚úÖ 03/2025 [Cluster and Predict Latent Patches for Improved Masked Image Modeling](https://arxiv.org/abs/2502.08769)
* üöÄ 03/2025 [Simplifying DINO via Coding Rate Regularization](https://arxiv.org/abs/2502.10385)
* ‚ùå [Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)
* ‚ùå [Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results](https://arxiv.org/abs/1703.01780)
* ‚ùå [iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832)
* ‚ùå [Emerging Properties in Self-Supervised Vision Transformers](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)
* ‚ùå [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/abs/2503.19903v1)
* ‚ùå [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
* ‚ùå [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)

### 3D Computer Vision
* ‚úÖ 03/2025: [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2503.11651)
* ‚úÖ 03/2025: [DKM: Dense Kernelized Feature Matching for Geometry Estimation](https://arxiv.org/abs/2202.00667)
* ‚úÖ 03/2025: [Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors](https://arxiv.org/abs/2503.17316)
* ‚úÖ 03/2025: [DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection](https://arxiv.org/abs/2503.07347)
* ‚ùå [MVSAnywhere: Zero-Shot Multi-View Stereo](https://nianticlabs.github.io/mvsanywhere/resources/MVSAnywhere.pdf)
* ‚ùå [Feat2GS: Probing Visual Foundation Models with Gaussian Splatting](https://arxiv.org/abs/2412.09606)
* ‚ùå [RoMa: Robust dense feature matching](https://arxiv.org/abs/2305.15404)
* ‚ùå [DeDoDe: Detect, don‚Äôt describe‚ÄîDescribe, don‚Äôt detect for local feature matching](https://arxiv.org/abs/2308.08479)
* ‚ùå [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)
* ‚ùå [MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion](https://arxiv.org/abs/2409.19152)

### General Computer Vision
* ‚úÖ 03/2025 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* ‚úÖ 03/2025 [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905)
* ‚ùå [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
* ‚ùå [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
* ‚ùå [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)
* ‚ùå [OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels](https://arxiv.org/abs/2502.20087v2)

### Geometric Deep Learning
* ‚ùå [Graph Attention Networks](https://arxiv.org/abs/1710.10903)

### NLP 
* ‚ùå [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### General Deep Learning 
* ‚ùå [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
* ‚ùå [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
* ‚ùå [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

