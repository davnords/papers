# Papers
I like reading about Deep Learning and Computer Vision. Here I simply list the papers I read and that I want to read. Inspiration: [here](https://github.com/fregu856/papers).

## Taxonomy
I categorize papers by those I have read and those I have yet to read. Later, I will classify them based on if they are classics or not. Right now I am reading a mix of new and time tested papers.

‚úÖ Paper I have read.

üöÄ Exceptional read.

‚ùå Paper I have yet to read.

----

## List

### Self-supervised learning in Computer Vision
* ‚úÖ 03/2025: [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
* ‚úÖ 03/2025: [Cluster and Predict Latent Patches for Improved Masked Image Modeling](https://arxiv.org/abs/2502.08769)
* üöÄ 03/2025: [Simplifying DINO via Coding Rate Regularization](https://arxiv.org/abs/2502.10385)
* ‚úÖ 04/2025: [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)
* ‚úÖ 04/2025: [SimMIM: A Simple Framework for Masked Image Modeling](https://arxiv.org/abs/2111.09886)
* ‚úÖ 05/2025: [iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832)
* ‚úÖ 06/2025: [Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)
* ‚úÖ 09/2025: [Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results](https://arxiv.org/abs/1703.01780)
* ‚ùå [Emerging Properties in Self-Supervised Vision Transformers](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)
* ‚úÖ 09/2025: [Scaling Vision Pre-Training to 4K Resolution](https://arxiv.org/abs/2503.19903v1)
* ‚ùå [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)
* ‚ùå [MultiMAE: Multi-modal Multi-task Masked Autoencoders](https://arxiv.org/abs/2204.01678)
* ‚ùå [MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments](https://arxiv.org/abs/2307.09361)
* ‚ùå [Scaling Language-Free Visual Representation Learning](https://arxiv.org/abs/2504.01017)
* ‚ùå [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
* üöÄ 06/2025: [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
* ‚ùå [Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture](https://arxiv.org/pdf/2301.08243)
* ‚ùå [Revisiting Feature Prediction for Learning Visual Representations from Video](https://arxiv.org/abs/2404.08471)
* ‚úÖ 06/2025: [Beyond [cls]: Exploring the true potential of Masked Image Modeling representations](https://arxiv.org/abs/2412.03215)
* ‚ùå [Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning](https://arxiv.org/abs/2505.12477)
* ‚ùå [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)

### 3D Computer Vision
* ‚úÖ 03/2025: [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2503.11651)
* ‚úÖ 03/2025: [DKM: Dense Kernelized Feature Matching for Geometry Estimation](https://arxiv.org/abs/2202.00667)
* ‚úÖ 03/2025: [Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors](https://arxiv.org/abs/2503.17316)
* ‚úÖ 03/2025: [DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection](https://arxiv.org/abs/2503.07347)
* ‚ùå [MVSAnywhere: Zero-Shot Multi-View Stereo](https://nianticlabs.github.io/mvsanywhere/resources/MVSAnywhere.pdf)
* ‚ùå [Feat2GS: Probing Visual Foundation Models with Gaussian Splatting](https://arxiv.org/abs/2412.09606)
* ‚ùå [RoMa: Robust dense feature matching](https://arxiv.org/abs/2305.15404)
* ‚úÖ 06/2025: [DeDoDe: Detect, don‚Äôt describe‚ÄîDescribe, don‚Äôt detect for local feature matching](https://arxiv.org/abs/2308.08479)
* ‚ùå [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)
* ‚ùå [MASt3R-SfM: a Fully-Integrated Solution for Unconstrained Structure-from-Motion](https://arxiv.org/abs/2409.19152)
* ‚ùå [Feat2GS: Probing Visual Foundation Models with Gaussian Splatting](https://arxiv.org/abs/2412.09606)
* ‚ùå [AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains Into One](https://arxiv.org/abs/2312.06709)
* ‚ùå [RayZer: A Self-supervised Large View Synthesis Model](https://arxiv.org/abs/2505.00702)
* ‚ùå [Cross-View Completion Models are Zero-shot Correspondence Estimators](https://arxiv.org/abs/2412.09072)
* ‚ùå [Test3R: Learning to Reconstruct 3D at Test Time](https://arxiv.org/abs/2506.13750)
* ‚ùå [VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on Kilometer-scale Long RGB Sequences](https://arxiv.org/abs/2507.16443)
* ‚úÖ 09/2025: [FastVGGT: Training-Free Acceleration of Visual Geometry Transformer](https://arxiv.org/abs/2509.02560)
* ‚úÖ 09/2025: [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://www.arxiv.org/abs/2509.13414)
* ‚ùå [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/pdf/2510.03104)
* ‚ùå [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
* ‚ùå [SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration](https://arxiv.org/abs/2511.17750)
* ‚úÖ 12/2025 [True Self-Supervised Novel View Synthesis is Transferable](https://openreview.net/forum?id=aJJppqAm6r)
* ‚úÖ 12/2025: [DUNE: Distilling a Universal Encoder from heterogenous 2D and 3D teachers](https://openaccess.thecvf.com/content/CVPR2025/papers/Sariyildiz_DUNE_Distilling_a_Universal_Encoder_from_Heterogeneous_2D_and_3D_CVPR_2025_paper.pdf)
* ‚ùå [Uncalibrated Structure from Motion on a Sphere](https://openaccess.thecvf.com/content/ICCV2025/papers/Ventura_Uncalibrated_Structure_from_Motion_on_a_Sphere_ICCV_2025_paper.pdf)
* ‚ùå [Multi-View Pyramid Transformer](https://gynjn.github.io/MVP/)
* ‚úÖ 12/2025 [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/pdf/2512.08924)
* ‚ùå [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://any-4d.github.io/)
* ‚ùå [E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training](https://arxiv.org/abs/2512.10950)
* ‚ùå [On Geometric Understanding and Learned Data Priors in VGGT](https://arxiv.org/abs/2512.11508)
* ‚ùå [WildGaussians: 3D Gaussian Splatting in the Wild](https://arxiv.org/pdf/2407.08447)
* ‚ùå [Warp Consistency for Unsupervised Learning of Dense Correspondences](https://openaccess.thecvf.com/content/ICCV2021/papers/Truong_Warp_Consistency_for_Unsupervised_Learning_of_Dense_Correspondences_ICCV_2021_paper.pdf)
* ‚ùå [Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting](https://arxiv.org/pdf/2512.15508)
* ‚ùå [Multi-View Foundation Models](https://arxiv.org/pdf/2512.15708)
* http://arxiv.org/pdf/2407.08447

### GPU Programming
* ‚ùå [Inside NVIDIA GPUs: Anatomy of high performance matmul kernels](https://www.aleksagordic.com/blog/matmul)
* ‚ùå [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)

### General Computer Vision
* ‚úÖ 03/2025: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* ‚úÖ 03/2025: [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905)
* ‚úÖ 09/2025: [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
* ‚ùå [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
* ‚ùå [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020)
* ‚ùå [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)
* ‚ùå [OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels](https://arxiv.org/abs/2502.20087v2)
* ‚ùå [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)
* ‚ùå [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)
* ‚ùå [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)
* ‚ùå [ResMLP: Feedforward networks for image classification with data-efficient training](https://arxiv.org/abs/2105.03404)
* ‚ùå [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601)
* ‚ùå [Scaling Vision Transformers to 22 Billion Parameters](https://arxiv.org/abs/2302.05442)
* ‚úÖ [Better plain ViT baselines for ImageNet-1k](https://arxiv.org/pdf/2205.01580)
* ‚ùå [Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design](https://proceedings.neurips.cc/paper_files/paper/2023/file/3504a4fa45685d668ce92797fbbf1895-Paper-Conference.pdf)
* ‚úÖ 06/2025: [Vision Transformers Don't Need Trained Registers](https://www.arxiv.org/abs/2506.08010)
* ‚úÖ 06/2025: [Scaling Laws of Motion Forecasting and Planning A Technical Report](https://arxiv.org/pdf/2506.08228)
* ‚úÖ 09/2025: [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
* ‚ùå [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
* ‚ùå [Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers](https://arxiv.org/abs/2511.13945)
* https://arxiv.org/abs/2512.19941
* https://arxiv.org/pdf/2602.08626

### Image Genearation / Diffusion
* ‚úÖ 05/2025: [Consistency Models](https://arxiv.org/abs/2303.01469)
* ‚ùå [One-Minute Video Generation with Test-Time Training](https://arxiv.org/abs/2504.05298)
* ‚ùå [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937)
* ‚ùå [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841)
* ‚ùå [SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation](https://arxiv.org/pdf/2503.09641)
* https://arxiv.org/abs/2602.04770

### Geometric Deep Learning
* ‚ùå [Graph Attention Networks](https://arxiv.org/abs/1710.10903)

### NLP 
* ‚úÖ 04/2025: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
* ‚ùå [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)
* ‚úÖ 12/2025: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)
* ‚ùå [LLM architecture comparison (blog)](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison)

### General Deep Learning 
* ‚ùå [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
* ‚ùå [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
* ‚ùå [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)
* ‚ùå [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
* ‚ùå [On the Biology of LLMs](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)
* ‚ùå [Deep Learning is Not So Mysterious or Different](https://arxiv.org/pdf/2503.02113)
 
## Books
* ‚ùå [Understanding Deep Learning](https://udlbook.github.io/udlbook/)
* ‚ùå [Foundations of Computer Vision](https://visionbook.mit.edu/taxonomy.html#helmholtz-perception-as-inference)
* ‚ùå [Reinforcement Learning an Introduction](http://incompleteideas.net/book/RLbook2020.pdf)
* ‚ùå [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala)

## Blogs
* ‚ùå [Generative modelling in latent space](https://sander.ai/2025/04/15/latents.html)
* ‚ùå [Noise schedules considered harmful](https://sander.ai/2024/06/14/noise-schedules.html)
* ‚ùå [Diffusion is spectral autoregression](https://sander.ai/2024/09/02/spectral-autoregression.html)

## High performance computing
* ‚ùå [Anatomy of High-Performance Matrix Multiplication](https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf)
* ‚ùå [Inside vLLM: Anatomy of a High-Throughput LLM Inference System](https://www.aleksagordic.com/blog/vllm)
* ‚ùå [GPU glossary](https://modal.com/gpu-glossary)
