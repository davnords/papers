# Papers
Here I simply list the papers I read and that I want to read. Inspiration: (https://github.com/fregu856/papers)[here].

## Taxonomy
I categorize papers by those I have read and those I have yet to read.

✅ Paper I have read.

❌ Paper I have yet to read.

----

## 2025

### Self-supervised learning
* ✅ 03/2025 [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)
* ❌ [Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning](https://arxiv.org/abs/2006.07733)
* ❌ [Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-Supervised Deep Learning Results](https://arxiv.org/abs/1703.01780)
* ❌ [iBOT: Image BERT Pre-Training with Online Tokenizer](https://arxiv.org/abs/2111.07832)
* ❌ [Emerging Properties in Self-Supervised Vision Transformers](https://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html)

### 3D Computer Vision
* ✅ 03/2025: [VGGT: Visual Geometry Grounded Transformer](https://arxiv.org/abs/2503.11651)
* ❌ [DKM: Dense Kernelized Feature Matching for Geometry Estimation](https://arxiv.org/abs/2202.00667)
* ❌ [RoMa: Robust dense feature matching](https://arxiv.org/abs/2305.15404)
* ❌ [DeDoDe: Detect, don’t describe—Describe, don’t detect for local feature matching](https://arxiv.org/abs/2308.08479)

### General Computer Vision
* ✅ 03/2025 [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
* ❌ [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)
* ❌ [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)

### Geometric Deep Learning

### NLP 
* ❌ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### General Deep Learning 
* ❌ [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)
